{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataload and cleaning- since it is a continous valued dataset-i.e,iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data)\n",
    "df.columns = [\"sl\", \"sw\", 'pl', 'pw'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO find and assign labels for respective values of iris dataset since it is a continous valued dataset\n",
    "\n",
    "def label(val, *boundaries):\n",
    "    if (val < boundaries[0]):\n",
    "        return 'a'\n",
    "    elif (val < boundaries[1]):\n",
    "        return 'b'\n",
    "    elif (val < boundaries[2]):\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n",
    "\n",
    "\n",
    "def toLabel(df, old_feature_name):\n",
    "    second = df[old_feature_name].mean()\n",
    "    minimum = df[old_feature_name].min()\n",
    "    first = (minimum + second)/2\n",
    "    maximum = df[old_feature_name].max()\n",
    "    third = (maximum + second)/2\n",
    "    return df[old_feature_name].apply(label, args= (first, second, third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       sl   sw   pl   pw sl_labelled sw_labelled pl_labelled pw_labelled\n",
       "0    5.1  3.5  1.4  0.2           b           c           a           a\n",
       "1    4.9  3.0  1.4  0.2           a           b           a           a\n",
       "2    4.7  3.2  1.3  0.2           a           c           a           a\n",
       "3    4.6  3.1  1.5  0.2           a           c           a           a\n",
       "4    5.0  3.6  1.4  0.2           a           c           a           a\n",
       "..   ...  ...  ...  ...         ...         ...         ...         ...\n",
       "145  6.7  3.0  5.2  2.3           c           b           c           d\n",
       "146  6.3  2.5  5.0  1.9           c           a           c           d\n",
       "147  6.5  3.0  5.2  2.0           c           b           c           d\n",
       "148  6.2  3.4  5.4  2.3           c           c           d           d\n",
       "149  5.9  3.0  5.1  1.8           c           b           c           c\n",
       "\n",
       "[150 rows x 8 columns]>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert all columns to labelled data\n",
    "\n",
    "\n",
    "df['sl_labelled'] = toLabel(df, 'sl')\n",
    "df['sw_labelled'] = toLabel(df, 'sw')\n",
    "df['pl_labelled'] = toLabel(df, 'pl')\n",
    "df['pw_labelled'] = toLabel(df, 'pw')\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['sl', 'sw', 'pl', 'pw'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['sl_labelled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_flowers = ['Iris-Setosa(0)','Iris-versicolor(1)','Iris-Virginica(2)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO print the steps for decision tree-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operation(x):                     \n",
    "    return -1.0*x* math.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_gen(y):               #to find the unique values of y its frequency and store possible values in a dictionary\n",
    "    dict ={}\n",
    "    for i in y[0]:\n",
    "        dict[i]= dict.get(i,0) +1\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):             # to calculate the entropy \n",
    "    ans=0.0\n",
    "    dict= dict_gen(y)\n",
    "    tot = y.shape[0]\n",
    "    for i in dict:\n",
    "        ans+= operation(dict[i]/tot)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_ratio(ent, df, y, f): # to find the gain_ratio \n",
    "    poss_values = set(df[f])\n",
    "    tot = y.shape[0]\n",
    "    tot_ent = 0.0\n",
    "    split_ratio=0.0\n",
    "    for i in poss_values:\n",
    "        y_f= y[df[f]==i]\n",
    "        curr_cnt = y_f.shape[0]\n",
    "        curr_split= operation(curr_cnt/tot)\n",
    "        curr_ent = (curr_cnt*entropy(y_f))/tot\n",
    "        tot_ent+= curr_ent\n",
    "        split_ratio+=curr_split\n",
    "    info_gain = ent- tot_ent\n",
    "    return (info_gain/split_ratio)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree1(df, y, unused_features,lvl):   \n",
    "    print(\"Level \", lvl)\n",
    "    count = set(y[0])\n",
    "    z=list(count)\n",
    "    ent = entropy(y)\n",
    "    if(len(count)==1):                          #base case1 for unique value of y\n",
    "        print(\"Count of \",label_flowers[z[0]],\" = \", y.shape[0])\n",
    "        print(\"Current Entropy  is = 0.0\")\n",
    "        print(\"Reached leaf Node \\n\")\n",
    "        return\n",
    "    if(len(unused_features)==0):              #case when no features to split remain\n",
    "        dict =dict_gen(y)\n",
    "        for k in dict:\n",
    "            print(\"Count of \",label_flowers[k],\" = \", dict[k])\n",
    "        print(\"Current Entropy  is = \",ent)\n",
    "        print(\"Reached leaf Node \\n\")\n",
    "        return\n",
    "    \n",
    "    dict =dict_gen(y)\n",
    "    for i in dict:\n",
    "        print(\"Count of \",label_flowers[i] ,\" = \", dict[i] )\n",
    "\n",
    "    best_gain_ratio = 0.0 \n",
    "    best_feature = \"\"\n",
    "    for f in unused_features:                 # to find the feature which gives best gain ratio\n",
    "        gain_r = gain_ratio(ent,df,y,f)\n",
    "        if(gain_r >= best_gain_ratio):\n",
    "            best_gain_ratio = gain_r\n",
    "            best_feature = f\n",
    "      \n",
    "    print(\"Splitting on feature \", best_feature, \" with gain ratio \", best_gain_ratio, \" \\n\")\n",
    "    best_feat_val = set(df[best_feature])    \n",
    "    unused_features.remove(best_feature)    \n",
    "    for i in best_feat_val:                  \n",
    "        df_new=df[df[best_feature] == i]\n",
    "        y_new = y[df[best_feature] == i]\n",
    "        Tree1(df_new, y_new, unused_features,lvl+1)   \n",
    "                    \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(iris.target)\n",
    "unused_features = set(df.columns)\n",
    "X_test = df\n",
    "Y_test = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  0\n",
      "Count of  Iris-Setosa(0)  =  50\n",
      "Count of  Iris-versicolor(1)  =  50\n",
      "Count of  Iris-Virginica(2)  =  50\n",
      "Splitting on feature  pw_labelled  with gain ratio  0.6996382036222091  \n",
      "\n",
      "Level  1\n",
      "Count of  Iris-Setosa(0)  =  50\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n",
      "Level  1\n",
      "Count of  Iris-Virginica(2)  =  34\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n",
      "Level  1\n",
      "Count of  Iris-versicolor(1)  =  10\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n",
      "Level  1\n",
      "Count of  Iris-versicolor(1)  =  40\n",
      "Count of  Iris-Virginica(2)  =  16\n",
      "Splitting on feature  pl_labelled  with gain ratio  0.43340994956210654  \n",
      "\n",
      "Level  2\n",
      "Count of  Iris-Virginica(2)  =  8\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n",
      "Level  2\n",
      "Count of  Iris-versicolor(1)  =  1\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n",
      "Level  2\n",
      "Count of  Iris-versicolor(1)  =  39\n",
      "Count of  Iris-Virginica(2)  =  8\n",
      "Splitting on feature  sl_labelled  with gain ratio  0.1267450377580933  \n",
      "\n",
      "Level  3\n",
      "Count of  Iris-Virginica(2)  =  1\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n",
      "Level  3\n",
      "Count of  Iris-versicolor(1)  =  2\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n",
      "Level  3\n",
      "Count of  Iris-versicolor(1)  =  14\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n",
      "Level  3\n",
      "Count of  Iris-versicolor(1)  =  23\n",
      "Count of  Iris-Virginica(2)  =  7\n",
      "Splitting on feature  sw_labelled  with gain ratio  0.0709203640514889  \n",
      "\n",
      "Level  4\n",
      "Count of  Iris-versicolor(1)  =  3\n",
      "Count of  Iris-Virginica(2)  =  1\n",
      "Current Entropy  is =  0.5623351446188083\n",
      "Reached leaf Node \n",
      "\n",
      "Level  4\n",
      "Count of  Iris-versicolor(1)  =  14\n",
      "Count of  Iris-Virginica(2)  =  6\n",
      "Current Entropy  is =  0.6108643020548935\n",
      "Reached leaf Node \n",
      "\n",
      "Level  4\n",
      "Count of  Iris-versicolor(1)  =  6\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tree1(df, y, unused_features, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the functions into a class:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree:\n",
    "    def __init__(self,level):\n",
    "        self.level = level               \n",
    "        self.feature_splitted_upon = \"\"  \n",
    "        self.isleaf = False              \n",
    "        self.curr_feat_val = \"\"            \n",
    "        self.counts = {}                 \n",
    "        self.children = []              \n",
    "    \n",
    "    def fit_int(self,X,Y,unused_features): \n",
    "        count = set(Y[0])\n",
    "        z=list(count)\n",
    "        ent = entropy(Y)\n",
    "        if(len(count)==1):                    \n",
    "            self.isleaf = True\n",
    "            self.counts[z[0]] = Y.shape[0]\n",
    "            return\n",
    "        if(len(unused_features)==0):         \n",
    "            self.isleaf = True\n",
    "            dict =dict_gen(Y)\n",
    "            self.counts= dict\n",
    "            return\n",
    "    \n",
    "        dict =dict_gen(Y)\n",
    "        self.counts = dict                  \n",
    "        best_gain_ratio = 0.0 \n",
    "        best_feature = \"\"\n",
    "        for f in unused_features:           \n",
    "            gain_r = gain_ratio(ent,X,Y,f)\n",
    "            if(gain_r >= best_gain_ratio):\n",
    "                best_gain_ratio = gain_r\n",
    "                best_feature = f\n",
    "        \n",
    "        self.feature_splitted_upon = best_feature  \n",
    "        best_feat_val = set(X[best_feature])\n",
    "        unused_features.remove(best_feature)\n",
    "        for i in best_feat_val:\n",
    "            df_new=X[X[best_feature] == i]\n",
    "            y_new = Y[X[best_feature] == i]\n",
    "            obj =  Decision_Tree(self.level +1 )          \n",
    "            obj.fit_int(df_new, y_new, unused_features)   \n",
    "            obj.curr_feat_val = i\n",
    "            self.children.append(obj)                     \n",
    "        return \n",
    "\n",
    "    def fit(self,X,Y):                      \n",
    "        unused_features= set(X.columns)\n",
    "        self.fit_int(X,Y,unused_features)\n",
    "        \n",
    "    def predict_dum(self,X_test):       \n",
    "        if(self.isleaf == True):        \n",
    "            answer = \"\"\n",
    "            max_ans=0\n",
    "            for i in self.counts:\n",
    "                if(self.counts[i]>max_ans):\n",
    "                    max_ans = self.counts[i]\n",
    "                    answer = i\n",
    "            return answer\n",
    "        feat = self.feature_splitted_upon \n",
    "        answer=\"\"\n",
    "        for i in self.children:                   \n",
    "            if(X_test[feat]== i.curr_feat_val):   \n",
    "                answer= i.predict_dum(X_test)     \n",
    "                break\n",
    "        return answer                             \n",
    "    \n",
    "    def predict(self,X_test):                     \n",
    "        result = []\n",
    "        for i in range(X_test.shape[0]):          \n",
    "            k = X_test.iloc[i]\n",
    "            curr_ans = self.predict_dum(k)        \n",
    "            result.append(curr_ans)               \n",
    "        return result\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "curr_obj = Decision_Tree(0)          \n",
    "curr_obj.fit(X_test,Y_test)          \n",
    "pred_res = curr_obj.predict(X_test)  \n",
    "print(pred_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0  7 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.88      1.00      0.93        50\n",
      "           2       1.00      0.86      0.92        50\n",
      "\n",
      "    accuracy                           0.95       150\n",
      "   macro avg       0.96      0.95      0.95       150\n",
      "weighted avg       0.96      0.95      0.95       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparison between actual and predicted results \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "print(confusion_matrix(Y_test, pred_res))\n",
    "print(classification_report(Y_test,pred_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 57  0]\n",
      " [ 0  0 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      1.00      1.00        57\n",
      "           2       1.00      1.00      1.00        43\n",
      "\n",
      "    accuracy                           1.00       150\n",
      "   macro avg       1.00      1.00      1.00       150\n",
      "weighted avg       1.00      1.00      1.00       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris1 = datasets.load_iris() \n",
    "df1 = pd.DataFrame(iris1.data)\n",
    "df1.columns = [\"sl\", \"sw\", 'pl', 'pw'] \n",
    "def label(val, *boundaries):\n",
    "    if (val < boundaries[0]):\n",
    "        return '1.0'\n",
    "    elif (val < boundaries[1]):\n",
    "        return '2.0'\n",
    "    elif (val < boundaries[2]):\n",
    "        return '3.0'\n",
    "    else:\n",
    "        return '4.0'\n",
    "\n",
    "\n",
    "def toLabel(df1, old_feature_name):\n",
    "    second = df1[old_feature_name].mean()\n",
    "    minimum = df1[old_feature_name].min()\n",
    "    first = (minimum + second)/2\n",
    "    maximum = df1[old_feature_name].max()\n",
    "    third = (maximum + second)/2\n",
    "    return df1[old_feature_name].apply(label, args= (first, second, third))\n",
    "df1['sl_labelled'] = toLabel(df1, 'sl')\n",
    "df1['sw_labelled'] = toLabel(df1, 'sw')\n",
    "df1['pl_labelled'] = toLabel(df1, 'pl')\n",
    "df1['pw_labelled'] = toLabel(df1, 'pw')\n",
    "df1.drop(['sl', 'sw', 'pl', 'pw'], axis = 1, inplace = True)\n",
    "set(df1['sl_labelled'])\n",
    "label_flowers = ['Iris1-Setosa(0)','Iris1-versicolor(1)','Iris1-Virginica(2)']\n",
    "y1 = pd.DataFrame(iris1.target)\n",
    "unused_features = set(df1.columns)\n",
    "X1_test = df1\n",
    "Y1_test = y1\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf=DecisionTreeClassifier()\n",
    "clf.fit(X1_test,Y1_test)\n",
    "y_test_pred=clf.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "print(confusion_matrix(y_test_pred, pred_res))\n",
    "print(classification_report(y_test_pred,pred_res))\n",
    "\n",
    "\n",
    "#comparison with inbuilt sklearn decision tree classification algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
